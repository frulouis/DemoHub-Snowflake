{
 "metadata": {
  "orig_nbformat": 4,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "hex_info": {
   "author": "Fru Nde",
   "project_id": "e5e956fd-2ae5-4f3a-956e-c8b8b3951f84",
   "version": "draft",
   "exported_date": "Sun May 26 2024 21:01:21 GMT+0000 (Coordinated Universal Time)"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Snowflake Snowpark Setup\n\nImported necessary modules and functions from Snowflake Snowpark for data processing and machine learning tasks. Printed the Snowpark version for reference. ",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "id": "ecd82e57-8623-4f38-a1cc-7bbd08672b69"
  },
  {
   "cell_type": "markdown",
   "id": "658d4234-fd41-45c5-9f8c-6ca88a4ead13",
   "metadata": {
    "name": "cell36",
    "collapsed": false
   },
   "source": "[![Snowflake](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8ie8h8yM00XAN_Kx)](https://www.google.com/)\n"
  },
  {
   "cell_type": "code",
   "source": "from snowflake.snowpark.functions import *\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.types import IntegerType\nfrom snowflake.snowpark.types import Variant\nimport snowflake.snowpark.functions as F\nimport json\nfrom pprint import pprint\nimport pandas as pd\nfrom snowflake.snowpark.types import FloatType\nimport numpy as np\nimport snowflake.ml\nfrom matplotlib import pyplot as plt\nfrom snowflake.snowpark.functions import udf\nfrom snowflake.ml.feature_store.feature_store import FeatureStore, CreationMode\n\n\nfrom snowflake.snowpark import version\nprint(version.VERSION)\n",
   "metadata": {
    "name": "cell13",
    "language": "python",
    "collapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "76e4adbf-d50b-4deb-922e-7aed1506e522"
  },
  {
   "cell_type": "markdown",
   "id": "103c8b83-9238-4cbb-a486-f07682014c17",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "## Get Active Snowpark Session\nRetrieve and print the active Snowflake Snowpark session."
  },
  {
   "cell_type": "code",
   "id": "65e20f6b-88ea-47b6-bd2f-e65a9a8b3991",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "collapsed": false
   },
   "outputs": [],
   "source": "\n#Get Active Snowpark Session\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\nprint(session)\n     ",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "506ee29e-a65c-4af6-855b-cc2bd814069c",
   "metadata": {
    "name": "cell41",
    "collapsed": false
   },
   "source": "# PART 1. DATA & FEATURE ENGINEERING"
  },
  {
   "cell_type": "markdown",
   "id": "218bfc1c-e9bb-4743-b9f6-8882dd4718e6",
   "metadata": {
    "name": "cell42",
    "collapsed": false
   },
   "source": "[![Snowflake](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8ie8h8yM00XAN_Kx)](https://www.google.com/)"
  },
  {
   "cell_type": "markdown",
   "id": "11d7a199-c65c-42cd-ae76-d3c8f8c75040",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "## Load Data: Full Source Code\nFull source code: [IoTDB Tutorial](https://tutorials.demohub.dev/demo/iotdb).\n\n1. Created a database named `IoTDB`.\n2. Defined a file format named `CSV_SCHEMA` for CSV files with specific parsing settings.\n3. Created a stage named `DEMOHUB_S3_INT` referencing external data from 's3://demohubpublic/data/'.\n4. Created and populated a table named `sensor_data` using schema inference from CSV data in the stage.\n5. Loaded the actual data from the stage into the `sensor_data` table.\n"
  },
  {
   "cell_type": "code",
   "source": "--Load the Data: Full Source Code - https://tutorials.demohub.dev/demo/iotdb \n\n\n-- +----------------------------------------------------+\n-- |             1. DATABASE AND SCHEMA SETUP          |\n-- +----------------------------------------------------+\n\n-- Create or replace the database\nCREATE OR REPLACE DATABASE IoTDB;\n\n-- Use the database\nUSE IoTDB;\n\n-- +----------------------------------------------------+\n-- |            2. CREATE FILE FORMAT                 |\n-- +----------------------------------------------------+\n\n-- Create a file format to specify CSV structure\nCREATE OR REPLACE FILE FORMAT CSV_SCHEMA\n    TYPE = CSV\n    PARSE_HEADER = TRUE\n    SKIP_BLANK_LINES = TRUE\n    TRIM_SPACE = TRUE\n    ERROR_ON_COLUMN_COUNT_MISMATCH = FALSE;\n\n-- +----------------------------------------------------+\n-- |              3. CREATE STAGE                      |\n-- +----------------------------------------------------+\n\n-- Create a stage to reference external data\nCREATE OR REPLACE STAGE DEMOHUB_S3_INT \n    URL = 's3://demohubpublic/data/'\n    DIRECTORY = ( ENABLE = true )\n    COMMENT = 'DemoHub S3 datasets';\n\n-- +----------------------------------------------------+\n-- |        4. LOAD DATA USING SCHEMA INFERENCE        |\n-- +----------------------------------------------------+\n-- Create and populate the table using schema inference\nCREATE OR REPLACE TABLE sensor_data USING TEMPLATE (\nSELECT ARRAY_AGG(OBJECT_CONSTRUCT(*))\n    FROM TABLE (INFER_SCHEMA(\n    LOCATION=>'@demohub_s3_int/iot/sensor_data/',\n    FILE_FORMAT=>'CSV_SCHEMA')));\n\n-- +----------------------------------------------------+\n-- |                5. COPY DATA INTO TABLE             |\n-- +----------------------------------------------------+\n-- Load the actual data from the stage into the table\nCOPY INTO sensor_data FROM '@demohub_s3_int/iot/sensor_data/'\nFILE_FORMAT = 'CSV_SCHEMA'\nMATCH_BY_COLUMN_NAME = CASE_INSENSITIVE;",
   "metadata": {
    "name": "cell19",
    "language": "sql",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "304d44ed-e6e3-4083-aba6-cc26ed9879c7"
  },
  {
   "cell_type": "markdown",
   "id": "a92745ce-c6bf-4577-a9b2-6fbc1133bbb5",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "# ðŸ‘ Examine Target Value\nRetrieve the raw sensor data using a Snowpark DataFrame (not a Pandas DataFrame)."
  },
  {
   "cell_type": "code",
   "source": "raw_sensor_df = session.table(\"sensor_data\")",
   "metadata": {
    "name": "cell27",
    "language": "python",
    "collapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "eb4d0f07-a4f4-4551-8aa4-6b21d6867cb4"
  },
  {
   "cell_type": "code",
   "id": "ff3c46e2-71b9-4b4c-83f4-0b1855287b06",
   "metadata": {
    "language": "python",
    "name": "cell47",
    "collapsed": false
   },
   "outputs": [],
   "source": "raw_sensor_df",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1a2b89e0-ee88-42ae-a8bb-5fb7197e4c85",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "# ðŸ¤¯ Drop Uninformative Columns\nIdentify and remove columns with limited analytical value or variance.\n\n1. Get all column names from the raw sensor data.\n2. Specify columns to drop: \"TRA\", \"W31\", \"W32\".\n3. Select columns to keep based on those not in the drop list.\n4. Create a new DataFrame with only the retained columns.\n"
  },
  {
   "cell_type": "code",
   "source": "# ðŸ¤¯ Drop columns that have little to no analytical value/variance. \n\n# Get all column names\nall_columns = raw_sensor_df.columns\n\n# Columns to drop\ncolumns_to_drop = [\"TRA\", \"W31\", \"W32\"]\n\n# Select columns to keep\ncolumns_to_keep = [col for col in all_columns if col not in columns_to_drop]\n\n# Create a new DataFrame with only the kept columns\nraw_sensor_df = raw_sensor_df.select(*columns_to_keep)",
   "metadata": {
    "name": "cell24",
    "language": "python",
    "collapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "d1a1a240-5753-44ec-90f9-2075ecc3c7b7"
  },
  {
   "cell_type": "markdown",
   "id": "58560e05-e870-4268-bffb-c52d12a52c3f",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "# Convert to Pandas DataFrame\nDisplay the contents of the Snowpark DataFrame as a Pandas DataFrame.\n"
  },
  {
   "cell_type": "code",
   "id": "3760f93d-ce06-4fce-bab5-9bacb66d7cc9",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Convert the list of lists into a Pandas DataFrame\nraw_sensor_df",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d4d809c7-51f0-4021-a4de-015344c33307",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "# Visual Data Exploration\nExplore the data visually by plotting histograms for all continuous variables.\n\n1. Convert the Snowpark DataFrame to a Pandas DataFrame.\n2. Plot histograms for all continuous variables with 30 bins.\n3. Display the histograms with a figsize of 15x15.\n"
  },
  {
   "cell_type": "code",
   "source": "# Lets explore the data some more - visually. Do so by plotting histograms for all continuous variables\nsensor_pdf = raw_sensor_df.toPandas()\nsensor_pdf.hist(bins=30, figsize=(15,15))\nplt.show()",
   "metadata": {
    "name": "cell23",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "e8a46028-e952-4479-9940-71f20f2da054"
  },
  {
   "cell_type": "markdown",
   "id": "de29c6e3-1848-428b-9123-c76d268cafab",
   "metadata": {
    "name": "cell17",
    "collapsed": false
   },
   "source": "# Identify Low/No Variance Columns\n\nIdentify columns with low variance, as they have little to no impact on model prediction.\n\n- Get column names.\n- Compute variances for each column.\n- Transform the result into a Pandas DataFrame, melt it, and sort by variance values.\n"
  },
  {
   "cell_type": "code",
   "source": "#Identify Low/No Variance Columns - So, they can be dropped Low Variance columns have little to no impact on the predictive power of our model.\ntrain_cols = raw_sensor_df.columns\nvariance_cols = list(map(variance,list(map(col,train_cols))))\nraw_sensor_df.select(variance_cols).to_pandas().melt().sort_values('value')",
   "metadata": {
    "name": "cell39",
    "language": "python",
    "collapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "47251869-2e16-4d9b-b418-0b98bc113ccf"
  },
  {
   "cell_type": "markdown",
   "id": "256a58be-3eaa-46e8-b0a8-8d9c8c561666",
   "metadata": {
    "name": "cell18",
    "collapsed": false
   },
   "source": "# Drop Non-Useful Columns\n\nRemove columns that are not useful for model prediction.\n\n- Specify columns to remove: 'NF_DMD', 'PCNFR_DMD', 'P2', 'T2', 'FARB', 'EPR', 'W31', 'W32', 'UNIT_NUMBER'.\n- Drop the specified columns from the dataset.\n"
  },
  {
   "cell_type": "code",
   "source": "#Drop Non-Useful Columns\n#remove_cols = ['NF_DMD', 'PCNFR_DMD', 'P2', 'T2', 'FARB', 'EPR', 'W31', 'W32', 'UNIT_NUMBER']\nremove_cols = ['NF_DMD', 'PCNFR_DMD', 'P2', 'T2', 'FARB', 'EPR', 'W31', 'W32']\nraw_sensor_df=raw_sensor_df.drop(remove_cols)\n",
   "metadata": {
    "name": "cell40",
    "language": "python",
    "collapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "5f35dced-fe30-4ef5-8d17-ebcc62b79899"
  },
  {
   "cell_type": "markdown",
   "id": "fe26d8a0-ec31-47c8-b386-717e73443b88",
   "metadata": {
    "name": "cell14",
    "collapsed": false
   },
   "source": "# Examine Target Value\nDetermine the maximum time in cycles (max_tic) for each unit number.\n\n1. Group the raw sensor data by 'unit_number'.\n2. Aggregate the maximum 'time_in_cycles' as 'max_tic' for each group.\n3. Convert the result to a Pandas DataFrame for examination.\n"
  },
  {
   "cell_type": "code",
   "source": "#Lets Examine Our Target Value\nmax_cycles=raw_sensor_df.group_by('unit_number').agg(F.max(\"time_in_cycles\").alias(\"max_tic\"))\nmax_cycles.to_pandas()",
   "metadata": {
    "name": "cell29",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "85226888-495a-483e-aa3c-dcf947b1292f"
  },
  {
   "cell_type": "markdown",
   "id": "e76ae5dd-d310-4916-8e9d-ea8b26d7809f",
   "metadata": {
    "name": "cell15",
    "collapsed": false
   },
   "source": "# Re-arrange Column Order\nAdjust column order by moving 'unit_number' from the front to the end and add the 'max_tic' column.\n\n1. Get the column names of the raw sensor data.\n2. Pop 'unit_number' from the front.\n3. Reorder columns and append 'unit_number' and 'max_tic'.\n"
  },
  {
   "cell_type": "code",
   "source": "#Re-arrange Column orders. Pop Unit_number from front and move to end. Then add max_tic column\ncol_names=raw_sensor_df.columns\n\n#Display the Column names\ncol_names\n\n#Pop and Reorder\ncol_names.pop(0)\ncol_names = list(map(F.col, col_names))\ncol_names.append(raw_sensor_df.unit_number.alias(\"unit_number\"))\ncol_names.append(max_cycles.max_tic)\n\n#Display again\ncol_names",
   "metadata": {
    "name": "cell30",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "e7470297-f7f0-4ea5-928d-69e6ec4c1c2c"
  },
  {
   "cell_type": "markdown",
   "source": "# Data Manipulation and Filtering\n\n1. Join raw sensor data with 'max_cycles'.\n2. Calculate Remaining Useful Life (RUL).\n",
   "metadata": {
    "name": "cell31",
    "collapsed": false
   },
   "id": "da8b80b6-ad70-448c-9876-6e12c1ab6f1f"
  },
  {
   "cell_type": "code",
   "source": "raw_sensor_df=raw_sensor_df.join(max_cycles, max_cycles.unit_number == raw_sensor_df.unit_number,join_type = 'inner' )\\\n         .select(col_names)\\\n         .with_column(\"RUL\", F.col(\"MAX_TIC\")-F.col(\"TIME_IN_CYCLES\") )\\\n         .drop('MAX_TIC')\\\n         .filter(F.col(\"time_in_cycles\")>0)\n\nraw_sensor_df",
   "metadata": {
    "name": "cell32",
    "language": "python",
    "collapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "ff30ada1-e58e-47c3-9e1c-0b294aeea5d9"
  },
  {
   "cell_type": "markdown",
   "source": "# Describe Time in Cycles (TIC) and Remaining Useful Life (RUL)\n\nCompute descriptive statistics for 'time_in_cycles' and 'RUL'.\n\n1. Select columns 'time_in_cycles' and 'RUL'.\n2. Calculate statistics (mean, stddev, min, max, etc.).\n3. Display the descriptive statistics. \n",
   "metadata": {
    "name": "cell35",
    "collapsed": false
   },
   "id": "08687b83-5cd5-468c-a30d-e134f50cdfe6"
  },
  {
   "cell_type": "code",
   "source": "#Describe the TIC \nraw_sensor_df.select(['time_in_cycles','RUL']).describe().show()",
   "metadata": {
    "name": "cell37",
    "language": "python",
    "collapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "85aabfe0-d32a-4d8f-a639-797e524c2c53"
  },
  {
   "cell_type": "markdown",
   "id": "d509b0bc-6616-47ea-9280-c10fc3549daa",
   "metadata": {
    "name": "cell16",
    "collapsed": false
   },
   "source": "# Data Engineering: Deduplicate Dataset\n\nRemove duplicate rows from the dataset.\n\n1. Drop duplicate rows based on all columns.\n"
  },
  {
   "cell_type": "code",
   "source": "#Do some data engineering by De-duplicating the dataset\nraw_sensor_df=raw_sensor_df.drop_duplicates()",
   "metadata": {
    "name": "cell38",
    "language": "python",
    "collapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "783683d1-d7bd-4b69-8127-0e3e26994903"
  },
  {
   "cell_type": "markdown",
   "id": "f3bf1b1c-1ef4-4b1a-bd83-0c90dd947d82",
   "metadata": {
    "name": "cell20",
    "collapsed": false
   },
   "source": "# Create Feature Store for Up-to-date Features\n\nCreate a view named 'sensor_data_features' to store up-to-date features. \n\n- Use the Snowflake UI to verify successful creation of the view.\n"
  },
  {
   "cell_type": "code",
   "source": "#Create a View For Up-to-date features Check Snowflake UI to see that view has been created successfully\nraw_sensor_df.createOrReplaceView('sensor_data_features')",
   "metadata": {
    "name": "view",
    "language": "python",
    "collapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "94b8b21d-6719-4d3f-b237-3eaf9ae0fe77"
  },
  {
   "cell_type": "code",
   "id": "f4f21b9d-5e91-43fb-b3c0-e059d78a9616",
   "metadata": {
    "language": "python",
    "name": "cell45",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\nfs = FeatureStore(\n    session=session,\n    database=\"IOTDB\",\n    name=\"sensor_data_fs\",\n    default_warehouse=\"DEMO_WH\",\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e60b6077-0c9d-4f45-b953-1a2d55e4e725",
   "metadata": {
    "language": "python",
    "name": "cell46",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.feature_store import Entity\n\n# Example entity definition (adjust based on your data)\nsensor_entity = Entity(\n    name=\"SENSOR_ENTITY\",\n    join_keys=[\"UNIT_NUMBER\", \"TIME_IN_CYCLES\"] # Primary key or identifier for your sensor data\n)\n\nfs.register_entity(sensor_entity)\n\nfs.list_entities().show()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "21e6c053-dbea-49cb-83f3-aec23173970b",
   "metadata": {
    "language": "python",
    "name": "cell48",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.feature_store import FeatureView\n\nsensor_data_df = session.table(\"IOTDB.PUBLIC.SENSOR_DATA_FEATURES\")\n\n# Create the FeatureView object\nsensor_feature_view = FeatureView(\n    name=\"SENSOR_DATA_FEATURE_VIEW\",\n    entities=[sensor_entity],  # Include your entity if you defined one\n    feature_df=sensor_data_df,  # Select the columns to be features\n    #timestamp_col=\"TIMESTAMP_COL\",  # Assuming you have a timestamp column in your DataFrame\n    refresh_freq=None, #CHANGE AS NEEDED ****\n    desc=\"Feature view for sensor data\"\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f951df64-285a-4310-8200-e8e39aec7e72",
   "metadata": {
    "language": "python",
    "name": "cell58",
    "collapsed": false
   },
   "outputs": [],
   "source": "registered_fv: FeatureView = fs.register_feature_view(\n    feature_view=sensor_feature_view,    # feature view created above, could also use external_fv\n    version=\"1\",\n    block=True         # whether function call blocks until initial data is available\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aea45c16-2aeb-4322-acf4-f4f825545c25",
   "metadata": {
    "name": "cell21",
    "collapsed": false
   },
   "source": "# Create Model Stage in Snowflake\n\nCreate or replace a stage named 'model_stage' in Snowflake to store the trained model. \n\n- Use this stage to bring your own pre-trained model if needed.\n"
  },
  {
   "cell_type": "code",
   "id": "13f4b481-f246-4899-8a41-9951af66f8b0",
   "metadata": {
    "language": "python",
    "name": "cell4",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Create a Stage in Snowflake to Hold the Train Model. Note: Can use this stage to bring your own Pre-trained Model\nsession.sql('CREATE OR REPLACE STAGE  model_stage').show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# PART 2. MODEL TRAINING",
   "metadata": {
    "name": "cell49",
    "collapsed": false
   },
   "id": "b92b52e3-1498-4d1b-80f4-efc164d32081"
  },
  {
   "cell_type": "markdown",
   "id": "6c7d5b33-52c8-4fa0-aa15-88fd54f73dbf",
   "metadata": {
    "name": "cell43",
    "collapsed": false
   },
   "source": "[![Snowflake](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8ie8h8yM00XAN_Kx)](https://www.google.com/)"
  },
  {
   "cell_type": "markdown",
   "id": "cfb39a2a-e217-4529-bb20-847fe2a37378",
   "metadata": {
    "name": "cell22",
    "collapsed": false
   },
   "source": "# Define GBM Training Method\n\nDefine a method to train Gradient Boosting Machines (GBM) using the specified session, features table, and version number.\n\n- The method uses scikit-learn's GradientBoostingRegressor for training.\n- It splits the data into train and test sets and fits the model.\n- The trained model is then logged to the Snowflake ML Model Registry.\n- Returns model information and training performance metrics.\n"
  },
  {
   "cell_type": "code",
   "source": "#Define a METHOD to do the Gradient Boosting Machines Training\n\ndef train_time_to_fail_gbm(session:Session, features_table:str, version_num:str)-> Variant:\n    from sklearn.ensemble import GradientBoostingRegressor\n    from sklearn.model_selection import train_test_split\n    from snowflake.ml.registry import Registry  # Import from Snowflake ML library\n    import os\n    import datetime\n    from joblib import dump\n    \n    df_in = session.table(features_table)\n\n    #Use this option for reading from the feature store and using the dataset object. \n    #dataset = generate_fs_dataset(session, 'SENSOR_DATA_FEATURE_VIEW','SENSOR_DATA_FEATURE_VIEW', 'SENSOR_DATA_DATASET')\n    #training_df = dataset.read.to_pandas()\n   \n    \n    training_df = df_in.to_pandas()\n\n    \n    gbm = GradientBoostingRegressor()\n    \n    \n    X = training_df.iloc[:,:-1].to_numpy()\n    Y = training_df.iloc[:,-1:].to_numpy()\n    Y = np.ravel(Y)\n    \n\n    n_estimators = 200 \n    reg = GradientBoostingRegressor(random_state =0, verbose = True, max_depth =2, n_estimators = n_estimators)\n    \n    X_train, X_test, y_train, y_test = train_test_split(X,Y,random_state = 42, test_size = 0.1)\n    fit_model = reg.fit(X_train, y_train)\n\n    version_name = version_num\n    #version_name = datetime.datetime.now().strftime(\"v_%Y_%m_%d_%H_%M_%S\")\n    model_name=\"time_to_fail_model\"\n    \n    #utils_log_model(session=session, model=fit_model, model_name=\"time_to_fail_model\", input_example=X_train[:5], description=\"A GBM for IoT data predicting remaining useful life, and time to fail\")\n\n    registry = Registry(session=session)\n    # Log the model to the registry\n    model_info = registry.log_model(\n        model=fit_model,\n        model_name=model_name,\n        version_name=version_name,\n        sample_input_data=X_train[:5],\n        comment=\"A GBM for IoT data predicting remaining useful life, and time to fail\",\n        options={'relax_version': False}\n    )\n    return {\"Model_Name\": model_name,\"Model_Info\": model_info,\"R2_Train\": fit_model.score(X_train,y_train),\"R2_Test\":fit_model.score(X_test,y_test)}\n    #return {\"Model_Name\": model_name,\"Model_Version\": version_name,\"Model_Info\": model_info,\"R2_Train\": fit_model.score(X_train,y_train),\"R2_Test\":fit_model.score(X_test,y_test)}",
   "metadata": {
    "name": "cell50",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "91b164cc-7f67-4baa-9887-166670cd3965"
  },
  {
   "cell_type": "markdown",
   "source": "# Train Gradient Boost Model (GBM)\n\nTrain a Gradient Boosting Machine (GBM) model by initiating the Snowpark session locally in the notebook. \n\n- The code executes in Snowflake Snowpark environment.\n- Use the specified session and features table (\"SENSOR_DATA_FEATURES\"). \n",
   "metadata": {
    "name": "cell51",
    "collapsed": false
   },
   "id": "6316ec7e-82ed-4d5e-83e3-afde071d30ad"
  },
  {
   "cell_type": "code",
   "source": "#Train Gradient Boost Model (GBM) - By Intiating the Snowpark Session Locally In Notebook. The code runs in Snowflake Snowpark\ntrain_time_to_fail_gbm(session, \"SENSOR_DATA_FEATURES\", 'v1')",
   "metadata": {
    "name": "cell52",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "b2398c22-0d09-494e-a22d-a1daffaa2ffa"
  },
  {
   "cell_type": "markdown",
   "source": "# Register Gradient Boost Model Training Procedure\n\nRegister a stored procedure named \"train_time_to_fail_gbm_model\" in Snowflake.\n\n- The procedure trains the GBM model using the function \"train_time_to_fail_gbm\".\n- Necessary packages are included.\n- The trained model is stored in the stage location \"@model_stage\".  \n",
   "metadata": {
    "name": "cell53",
    "collapsed": false
   },
   "id": "86fc1ad2-5961-47d5-be92-dd2e3ecd0bf4"
  },
  {
   "cell_type": "code",
   "source": "#Train Gradient Boost Model On Snowflake - With A Storeproc That Can Be Called, Monitored, Governed or Orchestrated in Snowflake\nsession.sproc.register(func = train_time_to_fail_gbm, \\\n                       name = \"train_time_to_fail_gbm_model\",\\\n                       packages = ['snowflake-snowpark-python','scikit-learn','joblib','snowflake-ml'],\\\n                       is_permanent=True,stage_location=\"@model_stage\",\\\n                       replace=True)",
   "metadata": {
    "name": "cell54",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "18d28ead-40c7-4917-a376-1ca10678b6a5"
  },
  {
   "cell_type": "markdown",
   "id": "d408d356-7072-4f46-932e-161c83f72337",
   "metadata": {
    "name": "cell25",
    "collapsed": false
   },
   "source": "## Train Model Using Features Views\n\nPass the features view (\"SENSOR_DATA_FEATURES\") into the stored procedure to train the model.\n\n- Call the stored procedure \"train_time_to_fail_gbm_model\".\n- Provide the features view and version number (\"V13\") as inputs.\n"
  },
  {
   "cell_type": "code",
   "source": "## Passing the views (features) in to the SP to train the model \nsession.call('train_time_to_fail_gbm_model',\"SENSOR_DATA_FEATURES\", \"v2\")",
   "metadata": {
    "name": "cell55",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "d7d92ca6-6fbf-4a9f-84df-47af957c55f0"
  },
  {
   "cell_type": "markdown",
   "id": "d5377998-733f-45b7-b965-c9ee5c51a2af",
   "metadata": {
    "name": "cell34",
    "collapsed": false
   },
   "source": "## Training Time-to-Fail GBM Model in SQL\n\nTrains a time-to-fail GBM model on the `SENSOR_DATA_FEATURES` table from the `iotdb.public` schema. The model version is specified as `v50`, utilizing the `train_time_to_fail_gbm_model` stored procedure.\n"
  },
  {
   "cell_type": "code",
   "id": "c884954f-e7c4-408a-9785-e405dac8024a",
   "metadata": {
    "language": "sql",
    "name": "cell56",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "--- ðŸ’ª Task: Switch to Snowsight and Invoke the StoredProc In a Worksheet\n\ncall IOTDB.SENSOR_DATA_FS.TRAIN_TIME_TO_FAIL_GBM_MODEL('SENSOR_DATA_FEATURES','v3');",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "55fb4348-2b1c-4b20-9b0d-82ffe1c42071",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "# PART 3. USING MODELS FOR INFERENCE"
  },
  {
   "cell_type": "markdown",
   "id": "0ee16cc7-4657-4aea-92d4-51c83e983bc3",
   "metadata": {
    "name": "cell44",
    "collapsed": false
   },
   "source": "[![Snowflake](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8ie8h8yM00XAN_Kx)](https://www.google.com/)"
  },
  {
   "cell_type": "markdown",
   "id": "8ca615ab-ca36-4e8f-89a5-1cd392882fe3",
   "metadata": {
    "name": "cell26",
    "collapsed": false
   },
   "source": "## Selecting Actual and Predicted Time to Fail\n\nRetrieves actual and predicted time to fail from the `SENSOR_DATA_FEATURES` table. The `RUL` column is aliased as `ACTUAL_TIME_TO_FAIL`, while the prediction is generated using a time to fail model applied to various sensor features, resulting in the `PREDICTED_TIME_TO_FAIL` column.\n"
  },
  {
   "cell_type": "code",
   "id": "88fd7b40-b5b6-4817-8110-c8e0ac0f17fa",
   "metadata": {
    "language": "sql",
    "name": "cell6",
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT\n    UNIT_NUMBER,TIME_IN_CYCLES,\n    RUL as ACTUAL_TIME_TO_FAIL,\n    time_to_fail_model!predict(TIME_IN_CYCLES, SETTING_1, SETTING_2, T24, T30, T50, P15, P30, NF, NC, PS30, PHI, NRF, NRC, BPR, HTBLEED, UNIT_NUMBER):output_feature_0::INT AS PREDICTED_TIME_TO_FAIL\nFROM\n    SENSOR_DATA_FEATURES order by 1 asc, 2 asc;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fb12901b-93ae-470d-9998-a05b05273c97",
   "metadata": {
    "name": "cell33",
    "collapsed": false
   },
   "source": "## Creating Prediction Results View\n\nA view named `PREDICTION_RESULTS` is created to store actual and predicted time to fail. The `ACTUAL_TIME_TO_FAIL` column is derived from the `RUL` column, while `PREDICTED_TIME_TO_FAIL` is generated using a time to fail model applied to various sensor features from the `SENSOR_DATA_FEATURES` table.\n"
  },
  {
   "cell_type": "code",
   "source": "CREATE OR REPLACE VIEW PREDICTION_RESULTS AS\nSELECT\n    UNIT_NUMBER,TIME_IN_CYCLES,\n    RUL as ACTUAL_TIME_TO_FAIL,\n    time_to_fail_model!predict(TIME_IN_CYCLES, SETTING_1, SETTING_2, T24, T30, T50, P15, P30, NF, NC, PS30, PHI, NRF, NRC, BPR, HTBLEED, UNIT_NUMBER):output_feature_0::INT AS PREDICTED_TIME_TO_FAIL\nFROM\n    IOTDB.PUBLIC.SENSOR_DATA_FEATURES order by 1 asc, 2 asc;\n",
   "metadata": {
    "name": "cell66",
    "language": "sql",
    "collapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "3ada6eea-8ca5-4bd2-b217-6f23040c0cd7"
  },
  {
   "cell_type": "code",
   "source": "results_df = session.table(\"PREDICTION_RESULTS\").to_pandas()",
   "metadata": {
    "name": "cell68",
    "language": "python",
    "collapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "1b03a9dd-604d-4868-8e1c-837ab59a8b74"
  },
  {
   "cell_type": "markdown",
   "id": "f1dd7160-c87f-441b-a62b-eb063c6e2977",
   "metadata": {
    "name": "cell28",
    "collapsed": false
   },
   "source": "## Visualizing Actual vs. Predicted Time to Fail\n\nPlotting actual versus predicted time to fail using a scatter plot. Blue dots represent actual values, while orange dots represent predicted values. A diagonal reference line is added for comparison.\n"
  },
  {
   "cell_type": "code",
   "id": "870ecf60-1e97-4cb4-af03-b56cabb78371",
   "metadata": {
    "language": "python",
    "name": "cell57",
    "collapsed": false
   },
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\ndf = results_df\n\n# Plot actual vs predicted\nplt.figure(figsize=(10, 6))\nplt.scatter(df['ACTUAL_TIME_TO_FAIL'], df['PREDICTED_TIME_TO_FAIL'], alpha=0.5)\nplt.plot([df['ACTUAL_TIME_TO_FAIL'].min(), df['ACTUAL_TIME_TO_FAIL'].max()],\n         [df['ACTUAL_TIME_TO_FAIL'].min(), df['ACTUAL_TIME_TO_FAIL'].max()],\n         'r--', lw=2)\n\nplt.xlabel('Actual Time to Fail')\nplt.ylabel('Predicted Time to Fail')\nplt.title('Actual Time to Fail vs Predicted Time to Fail')\nplt.grid(True)\nplt.show()",
   "execution_count": null
  }
 ]
}